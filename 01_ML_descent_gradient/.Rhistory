data <- read.csv("data/4_1_data.csv")
data
data <- read.csv("data/4_1_data.csv")
plot(data$score.1, data$score.2, col = as.factor(data$label), xlab = "Score-1", ylab = "Score-2")
Sigmoid <- function(x) {
1 / (1 + exp(-x))
}
CostFunction <- function(parameters, X, Y) {
n <- nrow(X)
# function to apply (%*% Matrix multiplication)
g <- Sigmoid(X %*% parameters)
J <- (1/n) * sum((-Y * log(g)) - ((1 - Y) * log(1 - g)))
return(J)
}
# We want to minimize the cost function. Then derivate this funcion
TestGradientDescent <- function(iterations = 1200, X, Y) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
return(parameters)
}
TestGradientDescent(X=data[,1:2],Y=data[,3])
str(data)
var.exp <- as.matrix(data[,1:2])
var.dep <- as.matrix(data[,3])
TestGradientDescent(X=var.exp,Y=var.dep)
var.exp <- as.matrix(data[,1:2])
var.exp <- cbind(rep(1, nrow(var.exp)), var.exp)
var.dep <- as.matrix(data[,3])
TestGradientDescent(X=var.exp,Y=var.dep)
parameters <- TestGradientDescent(X=var.exp,Y=var.dep)
parameters
test_insample <- Sigmoid(t(var.exp) %*% parameters))
test_insample <- Sigmoid(t(var.exp) %*% parameters)
View(var.exp)
t(var.exp)
parameters
parameters <- as.matrix(parameters)
test_insample <- Sigmoid(t(var.exp) %*% parameters)
t(var.exp) %*% parameters
test_insample <- Sigmoid((t(var.exp)) %*% parameters)
test_insample <- Sigmoid( parameters %*%   t(var.exp))
test_insample <- Sigmoid((var.exp) %*% parameters)
test_insample
View(test_insample)
View(parameters)
hist(test_insample)
predict <- ifelse(test_insample > 0.65,1,0)
predict
results <- c(Y,predict)
results <- c(var.dep,predict)
results
results <- cbind(var.dep,predict)
results
colnames(results) <- c("Real","Predict")
table(results)
table(var.dep,predict)
seq(1:1200)
seq(0:1200)
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:1200)) {
parameters <- TestGradientDescent(iterations = i, X, Y)
convergence <- c(CostFunction(parameters, X, Y))
output <- cbind(i,round(convergence,2))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:1200)) {
parameters <- TestGradientDescent(iterations = i, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
output <- cbind(i,round(convergence,2))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
View(output)
View(iterations_error)
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:1200)) {
set.seed(12345)
parameters <- TestGradientDescent(iterations = i, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
output <- cbind(i,round(convergence,2))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:1200)) {
set.seed(12345)
parameters <- TestGradientDescent(iterations = i, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
output <- cbind(i,round(convergence,5))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
plot(iterations_error$convergence,iterations_error$iterations)
plot(iterations_error[,1],iterations_error[,2])
set.seed(12345)
parameters <- TestGradientDescent(iterations = 300, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
convergence
set.seed(12345)
parameters <- TestGradientDescent(iterations = 78, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
convergence
set.seed(12345)
parameters <- TestGradientDescent(iterations = 15, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
set.seed(123)
parameters <- TestGradientDescent(iterations = 15, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:1200)) {
set.seed(4567)
parameters <- TestGradientDescent(iterations = i, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
output <- cbind(i,round(convergence,5))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
View(iterations_error)
View(iterations_error)
iterations_error <- as.data.frame(iterations_error)
ggplot(iterations_error,aes(x=iterations_error$convergence,y=iterations_error$iterations)) + geom_line()
install.packages("ggplot2")
require(ggplot2)
ggplot(iterations_error,aes(x=iterations_error$convergence,y=iterations_error$iterations)) + geom_line()
View(iterations_error)
ggplot(iterations_error,aes(x=iterations_error$convergence,y=iterations_error$iterations)) + geom_point()
ggplot(iterations_error,aes(x=iterations_error$iterations,y=iterations_error$convergence)) + geom_point()
var.exp <- as.matrix(data[,1:2])
var.exp <- cbind(rep(1, nrow(var.exp)), var.exp)
var.dep <- as.matrix(data[,3])
parameters <- TestGradientDescent(X=var.exp,Y=var.dep)
test_insample <- Sigmoid((var.exp) %*% parameters)
cut_off <- 0.65
predict <- ifelse(test_insample > 0.65,1,0)
table(var.dep,predict)
i <- 0.65
cut_off <- i
predict <- ifelse(test_insample > i,1,0)
matrix_confusion <- table(var.dep,predict)
matrix_confusion[2,1]
error_type_1 <- matrix_confusion[2,1]/nrow(predict)
error_type_2 <- matrix_confusion[1,2]/nrow(predict)
cut_off <- cbind(0,0,0)
colnames(cut_off) <- c("cut_off","error_1","error_2")
for (i in seq(0,1,0.01)) {
predict <- ifelse(test_insample > i,1,0)
matrix_confusion <- table(var.dep,predict)
error_type_1 <- matrix_confusion[2,1]/nrow(predict)
error_type_2 <- matrix_confusion[1,2]/nrow(predict)
output <- cbind(i,error_type_1,error_type_2)
colnames(output) <- c("cut_off","error_1","error_2")
cut_off <- rbind(cut_off,output)
}
i
predict <- ifelse(test_insample > i,1,0)
predict
matrix_confusion <- table(var.dep,predict)
matrix_confusion
cut_off <- cbind(0,0,0)
colnames(cut_off) <- c("cut_off","error_1","error_2")
for (i in seq(0.1,0.99,0.01)) {
predict <- ifelse(test_insample > i,1,0)
matrix_confusion <- table(var.dep,predict)
error_type_1 <- matrix_confusion[2,1]/nrow(predict)
error_type_2 <- matrix_confusion[1,2]/nrow(predict)
output <- cbind(i,error_type_1,error_type_2)
colnames(output) <- c("cut_off","error_1","error_2")
cut_off <- rbind(cut_off,output)
}
View(cut_off)
cut_off <- cbind(0,0,0)
colnames(cut_off) <- c("cut_off","error_1","error_2","error_sum")
for (i in seq(0.1,0.99,0.01)) {
predict <- ifelse(test_insample > i,1,0)
matrix_confusion <- table(var.dep,predict)
error_type_1 <- matrix_confusion[2,1]/nrow(predict)
error_type_2 <- matrix_confusion[1,2]/nrow(predict)
output <- cbind(i,error_type_1,error_type_2,sum(error_type_1,error_type_2))
colnames(output) <- c("cut_off","error_1","error_2","error_sum")
cut_off <- rbind(cut_off,output)
}
cut_off <- cbind(0,0,0,0)
colnames(cut_off) <- c("cut_off","error_1","error_2","error_sum")
for (i in seq(0.1,0.99,0.01)) {
predict <- ifelse(test_insample > i,1,0)
matrix_confusion <- table(var.dep,predict)
error_type_1 <- matrix_confusion[2,1]/nrow(predict)
error_type_2 <- matrix_confusion[1,2]/nrow(predict)
output <- cbind(i,error_type_1,error_type_2,sum(error_type_1,error_type_2))
colnames(output) <- c("cut_off","error_1","error_2","error_sum")
cut_off <- rbind(cut_off,output)
}
View(cut_off)
data
table(data)
table(data$label)
data <- read.csv("data/4_1_data.csv")
plot(data$score.1, data$score.2, col = as.factor(data$label), xlab = "Score-1", ylab = "Score-2")
# Sigmoid function
Sigmoid <- function(x) {
1 / (1 + exp(-x))
}
# Plotting example
x <- seq(-5, 5, 0.01)
# and plot
plot(x, Sigmoid(x), col='blue', ylim = c(-.2, 1))
abline(h = 0, v = 0, col = "gray60")
# Ref: https://www.r-bloggers.com/logistic-regression-with-r-step-by-step-implementation-part-2/
# Cost Function
#
CostFunction <- function(parameters, X, Y) {
n <- nrow(X)
# function to apply (%*% Matrix multiplication)
g <- Sigmoid(X %*% parameters)
J <- (1/n) * sum((-Y * log(g)) - ((1 - Y) * log(1 - g)))
return(J)
}
# We want to minimize the cost function. Then derivate this funcion
TestGradientDescent <- function(iterations = 1200, X, Y) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
return(parameters)
}
var.exp <- as.matrix(data[,1:2]) # Explanatory variables
var.exp <- cbind(rep(1, nrow(var.exp)), var.exp) # Explanatory variables + constant
var.dep <- as.matrix(data[,3]) # dependent variable
parameters <- TestGradientDescent(X=var.exp,Y=var.dep) # optimized parameters w1,
parameters
```{r}
test_insample <- Sigmoid((var.exp) %*% parameters) # predicted dependent values
cut_off <- 0.39 # selected cut-off
predict <- ifelse(test_insample > 0.39,1,0)
table(var.dep,predict)
test_insample <- Sigmoid((var.exp) %*% parameters) # predicted dependent values
cut_off <- 0.39 # selected cut-off
predict <- ifelse(test_insample > 0.39,1,0)
real <- var.dep
table(real,predict)
iterations = 1200
iterations_vector = seq(1:iterations)
iterations_vector
parameters <- apply(iterations_vector,1,TestGradientDescent(iterations = iterations_vector , var.exp, var.dep))
var.exp
var.de`p`
var.dep
parameters <- lapply(iterations_vector,1,TestGradientDescent(iterations = iterations_vector , var.exp, var.dep))
parameters <- lapply(iterations_vector,margin=1,TestGradientDescent(iterations = iterations_vector , var.exp, var.dep))
# We want to minimize the cost function. Then derivate this funcion
TestGradientDescent <- function(iterations = 1200, X, Y) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
return(parameters)
}
parameters <- lapply(iterations_vector,margin=1,TestGradientDescent(iterations = iterations_vector , var.exp, var.dep))
parameters <- lapply(iterations_vector,margin=1,TestGradientDescent( X=var.exp, Y=var.dep))
iterations_vector = seq(1:iterations)
parameters <- lapply(iterations_vector,margin=1,TestGradientDescent( X=var.exp, Y=var.dep))
X
TestGradientDescent( X=var.exp, Y=var.dep)
iterations_costfunction <- function(iterations = 1200, X, Y) {
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:iterations)) {
parameters <- TestGradientDescent(iterations = i, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
output <- cbind(i,round(convergence,5))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
return(iterations_error)}
iterations_costfunction(1100,var.exp,var.dep)
iterations_costfunction <- function(iterations = 1200, X, Y) {
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:iterations)) {
parameters <- TestGradientDescent(iterations = i, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
output <- cbind(i,round(convergence,5))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
return(as.data.frame(iterations_error[-1,]))}
costfunctions_evolution  <- iterations_costfunction(var.exp,var.dep)
costfunctions_evolution  <- iterations_costfunction(X = var.exp, Y = var.dep)
ggplot(costfunctions_evolution,aes(x=costfunctions_evolution$iterations,y=costfunctions_evolution$convergence)) + geom_point()
iterations_costfunction <- function(iterations = 1200,X, Y) {
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:iterations)) {
parameters <- TestGradientDescent(iterations = i, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
output <- cbind(i,round(convergence,5))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
return(iterations_error[-1,])}
costfunctions_evolution  <- iterations_costfunction(X = var.exp, Y = var.dep)
plot(iterations_error[-1,1],iterations_error[-1,2])
plot(iterations_error[-1,1],iterations_error[-1,2])
iterations_costfunction <- function(iterations = 1200,X, Y) {
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:iterations)) {
parameters <- TestGradientDescent(iterations = i, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
output <- cbind(i,round(convergence,5))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
return(plot(iterations_error[-1,1],iterations_error[-1,2]))}
costfunctions_evolution  <- iterations_costfunction(X = var.exp, Y = var.dep)
iterations_costfunction <- function(iterations = 1200,X, Y) {
iterations_error <- cbind(0,0)
colnames(iterations_error) <- c("iterations","convergence")
for (i in seq(1:iterations)) {
parameters <- TestGradientDescent(iterations = i, var.exp, var.dep)
convergence <- c(CostFunction(parameters, var.exp, var.dep))
output <- cbind(i,round(convergence,5))
colnames(output) <- c("iterations","convergence")
iterations_error <- rbind(iterations_error,output)
}
return(plot(iterations_error[-1,1],iterations_error[-1,2]))}
costfunctions_evolution  <- iterations_costfunction(X = var.exp, Y = var.dep)
TestGradientDescent_2 <- function(iterations = 1200, X, Y) {
parameters <- rep(0, ncol(X))
# removed: print(paste("Initial Cost Function value: ", convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
parameters <- parameters_optimization$par
# removed: print(paste("Final Cost Function value: ",convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
return(parameters)
}
costfunctions_evolution  <- iterations_costfunction(X = var.exp, Y = var.dep) # applying the function to our data
iterations_error
View(iterations_error)
View(iterations_error)
